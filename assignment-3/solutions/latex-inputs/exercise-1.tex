\section{Exercise 1}

No ``final solutions'' are provided for these types of questions, since the
whole point of them is to encourage you to express \emph{briefly} but
\emph{clearly} and \emph{in your own words} what you understand. As
explained in the directions, definitions taken from text books or the
internet do not reflect a good understanding of these terms, nor do
extremely long explanations. Equations do not express the meaning of these,
nor do literal word translations of equations show that you know what they
mean. Instead, we are looking for clear evidence that you understand what
each term means. Possible definitions for each term is provided below: 

\begin{mdframed}[style=MyFrame]
    \begin{enumerate}[label=(\alph*)]
        \item Gram-Schmidt: The Gram-Schmidt process takes a set of linear
            independent vectors and constructs an orthonormal basis  that
            spans the same subspace. 
        \item $QR$ factorization: The $QR$ factorization procedure that
            starts with a matrix $A$ and factors into a orthogonal matrix
            $Q$ and an upper matrix $R$.
        \item Orthogonal subspaces and matrices: A orthogonal
            subspaces means that every $v$ in $V$ is orthogonal to every
            $w$ in $W$; a orthogonal matrix is a square matrix with
            orthornormal columns so that the transpose of the matrix is
            equal to its inverse.  
        \item Least squares approximations: The least squares method is a
            statistical procedure to find the best for a set of data point
            by minimizing the sum of the 
        \item Projection vector: The vector projection of a vector
            $\mathbf{a}$ on a nonzero vector $\mathbf{b}$ is the orthogonal
            projection of $\mathbf{a}$ onto a straight line parallel to
            $\mathbf{b}$.
    \end{enumerate}
\end{mdframed}
